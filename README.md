# Acapella-Powered Song Recaster
![Python](https://img.shields.io/badge/Python-3.12-blue)
![License](https://img.shields.io/badge/License-MIT-green)

Recreate any song using only mouth-made sounds through an end-to-end modular audio pipeline.

## Why?
Iâ€™ve always been a huge fan of acapella, and I wondered: **is it possible to recreate any song using only mouth-made sounds?** Well this project answers that question. 

## Pipeline
This project builds a pipeline that:

1. **Ingests** hundreds of acapella â€œinstrumentalâ€ tracks  
2. **Slices** them into short segments  
3. **Embeds** each slice with a pretrained audio model  
4. **Indexes** those embeddings with FAISS  
5. **Converts** any input song by matching its instrumental sections to your acapella library, time-stretching and stitching together a brand-new acapella â€œinstrumental,â€ then layering back the original vocals


## Features

- **Modular pipeline**: clear separation of data ingestion, embedding, matching, synthesis, and orchestration  
- **Pretrained embeddings**: uses Wav2Vec2 for robust audio features  
- **Fast nearest-neighbor search** with FAISS  
- **Time-stretching & cross-fade assembly** to avoid pops and align durations  
- **CLI tools** for ingestion, index building/updating, and conversion  

---
## Repo Structure
```
audiorecast/
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ src/
â”‚   â””â”€â”€ audiorecast/
â”‚       â”œâ”€â”€ data/
â”‚       â”‚   â”œâ”€â”€ downloader.py           # YouTube â†’ WAV
â”‚       â”‚   â”œâ”€â”€ stem_splitter.py        # Demucs wrapper & 4â†’2 stem merge
â”‚       â”‚   â””â”€â”€ segmenter.py            # Chop stems into fixed windows
â”‚       â”œâ”€â”€ embedding/
â”‚       â”‚   â””â”€â”€ extractor.py            # Wav2Vec2 feature extractor
â”‚       â”œâ”€â”€ matching/
â”‚       â”‚   â”œâ”€â”€ index_builder.py        # Build embeddings.npy + FAISS index
â”‚       â”‚   â””â”€â”€ matcher.py              # Query FAISS for top-k matches
â”‚       â”œâ”€â”€ synthesis/
â”‚       â”‚   â”œâ”€â”€ synthesizer.py          # Time-stretch pads slices
â”‚       â”‚   â””â”€â”€ assembler.py            # Cross-fade & concatenate
â”‚       â””â”€â”€ pipeline.py                 # End-to-end conversion logic
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_acapellas.py             # Download, split, slice
â”‚   â”œâ”€â”€ build_faiss_index.py            # Full rebuild
â”‚   â””â”€â”€ convert_song.py                 # Given input â†’ acapella-style output
â””â”€â”€ data/
    â”œâ”€â”€ ingest/                         # .txt playlists of YouTube URLs
    â”œâ”€â”€ raw/                            # Downloaded WAVs + stems + slices
    â””â”€â”€ embeddings/                     # embeddings.npy, paths.jsonl, .faiss

```

## Demo

**Input Song**: ğŸ§ `Stereo_Hearts_acapella.mp3`  
**Output (acapella-recast)**: ğŸ¤ `Stereo_Hearts_acapella.wav`

Spectrogram comparison:

![Original Spectrogram](tests/original_spec.png)
![Converted Spectrogram](tests/converted_spec.png)

Obviously, the results arenâ€™t perfect, yet. With a larger acapella database, better matching heuristics (re-ranking matches with cosine similarity + timing loss), and future improvements in synthesis, the output quality will significantly improve.

This is just the starting point, there's a lot of exciting room to grow.

## Docker Support

You can run this project directly via Docker:

**Build locally**:
```bash
docker build -t audiorecast .
```

## Installation Instructions
### Clone the repo
```bash
git clone https://github.com/Codingisinmyblud/audiorecast.git && cd audiorecast
```
### Install Dependencies

1. Install Poetry (if you don't have it yet):  
   [https://python-poetry.org/docs/#installation](https://python-poetry.org/docs/#installation)

2. Install project dependencies:  
```bash
poetry install
```
## Tech Stack
### Built With
- Python 3.12
- [FAISS](https://github.com/facebookresearch/faiss)
- [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base)
- [Demucs](https://github.com/facebookresearch/demucs)

## Future Improvements

This project is just the beginning. Here are some directions I plan to explore:

- Expand the acapella database for more diverse sounds
- Improve matching using music-specific audio embeddings (e.g. CLAP or MERT)
- Add beat-aware slicing and better tempo alignment (dynamic time warping, etc.)
- Explore smarter re-ranking (combine cosine similarity with timing constraints)
- Speed up pipeline using multiprocessing or batch inference
- Build a web interface for real-time previews and remixing

Have more ideas? Open an issue or PR, contributions are welcome!


## Contributing
Pull requests welcome. For major changes, open an issue first.

1. Fork the repo
2. Create your feature branch (`git checkout -b feature/foo`)
3. Commit your changes (`git commit -m 'Add foo'`)
4. Push and create a PR

## License
This project is licensed under the MIT License. See `LICENSE` for more info.
